# BERT: Bidirectional Encoding

## Introduction

BERT (Bidirectional Encoder Representations from Transformers) revolutionized NLP by introducing powerful pre-trained language representations that can be fine-tuned for various tasks.

## Core Concepts

### What Makes BERT Special?

1. **Bidirectional**: Looks at context from both left and right simultaneously
2. **Pre-trained**: Learns general language understanding from massive text
3. **Fine-tunable**: Adapts to specific tasks with minimal modification

### BERT Architecture

```
Input: [CLS] The movie was great [SEP]
         ↓
   Token Embeddings
   + Position Embeddings  
   + Segment Embeddings
         ↓
   12 Transformer Encoder Layers
         ↓
   Contextual Representations
```

### Pre-training Objectives

**1. Masked Language Modeling (MLM)**

```
Input:  "The [MASK] sat on the mat"
Target: "cat"

# 15% of tokens are masked
# Model predicts original token
```

**2. Next Sentence Prediction (NSP)**

```
Sentence A: "The cat sat on the mat."
Sentence B: "It was a sunny day."
Label: NotNextSentence

Sentence A: "The cat sat on the mat."
Sentence B: "It purred contentedly."
Label: IsNextSentence
```

### Using BERT with Transformers Library

```python
from transformers import BertTokenizer, BertModel
import torch

# Load pre-trained model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenize input
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors='pt')

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)
    
# outputs.last_hidden_state: (batch, seq_len, hidden_size)
# outputs.pooler_output: [CLS] token representation
```

### Fine-tuning for Classification

```python
from transformers import BertForSequenceClassification

# Load model for classification
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # binary classification
)

# Training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

for batch in train_loader:
    outputs = model(**batch, labels=batch['labels'])
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

### BERT Variants

| Model | Parameters | Use Case |
|-------|------------|----------|
| BERT-base | 110M | General purpose |
| BERT-large | 340M | Better accuracy |
| DistilBERT | 66M | Faster inference |
| RoBERTa | 125M | Better pre-training |

### Common Applications

- **Sentiment Analysis**: Classify text sentiment
- **Named Entity Recognition**: Identify entities in text
- **Question Answering**: Extract answers from context
- **Text Similarity**: Compare semantic similarity

---

## Key Points

- BERT uses bidirectional context (unlike GPT's left-to-right)
- Pre-trained on masked language modeling and next sentence prediction
- Fine-tuning adapts BERT to specific tasks
- [CLS] token provides sentence-level representation
- Multiple variants balance size and performance

---

## Reflection Questions

1. **Think**: Why is bidirectional context important? What limitations does left-to-right modeling have?

2. **Consider**: How does masked language modeling help BERT understand language? Why mask 15% of tokens?

3. **Explore**: When would you use BERT vs GPT? What are the tradeoffs between bidirectional and autoregressive models?
