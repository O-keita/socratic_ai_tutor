# SHAP Values for Model Explainability

## Introduction

SHAP (SHapley Additive exPlanations) provides a unified approach to explaining machine learning model predictions based on game theory concepts.

## Core Concepts

### What are SHAP Values?

SHAP values answer: "How much did each feature contribute to this specific prediction?"

Based on Shapley values from cooperative game theory:
- Fairly distribute the "payout" (prediction) among "players" (features)
- Consider all possible feature combinations

### The SHAP Equation

$$f(x) = \phi_0 + \sum_{i=1}^{M} \phi_i$$

Where:
- $f(x)$ is the model prediction
- $\phi_0$ is the base value (average prediction)
- $\phi_i$ is the SHAP value for feature i

### Using SHAP in Python

```python
import shap
import xgboost as xgb

# Train a model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

# Create explainer
explainer = shap.TreeExplainer(model)

# Calculate SHAP values
shap_values = explainer.shap_values(X_test)

# For a single prediction
shap.force_plot(explainer.expected_value, 
                shap_values[0], 
                X_test.iloc[0])
```

### Visualization Types

**1. Force Plot (Single Prediction)**
```python
# Shows how features push prediction from base value
shap.force_plot(explainer.expected_value, 
                shap_values[0], 
                X_test.iloc[0],
                feature_names=feature_names)
```

**2. Summary Plot (Global Importance)**
```python
# Shows feature importance across all predictions
shap.summary_plot(shap_values, X_test)
```

**3. Dependence Plot**
```python
# Shows relationship between feature value and SHAP value
shap.dependence_plot("feature_name", shap_values, X_test)
```

**4. Waterfall Plot**
```python
# Detailed breakdown of single prediction
shap.waterfall_plot(shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_test.iloc[0],
    feature_names=feature_names
))
```

### Interpreting SHAP Values

```python
# Positive SHAP value → increases prediction
# Negative SHAP value → decreases prediction

# Example for loan approval:
# Age SHAP = +0.3 → Age increased approval probability
# Income SHAP = -0.1 → Income decreased approval probability
```

### SHAP for Different Models

```python
# Tree models (fast)
explainer = shap.TreeExplainer(model)

# Deep learning
explainer = shap.DeepExplainer(model, background_data)

# Any model (slower)
explainer = shap.KernelExplainer(model.predict, background_data)
```

### Practical Example

```python
# Explain a loan rejection
idx = 42  # rejected application
print(f"Prediction: {model.predict_proba(X_test.iloc[idx:idx+1])[0]}")

# Get SHAP values
shap_vals = explainer.shap_values(X_test.iloc[idx:idx+1])

# Top factors for rejection
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'shap_value': shap_vals[0]
}).sort_values('shap_value')

print("Factors decreasing approval:")
print(feature_importance.head())
```

---

## Key Points

- SHAP values explain individual predictions fairly
- Based on game theory (Shapley values)
- Positive values increase prediction, negative decrease
- Multiple visualization types for different insights
- Model-agnostic but faster implementations for tree models

---

## Reflection Questions

1. **Think**: Why is SHAP considered "fair" in distributing feature contributions? What properties does it satisfy?

2. **Consider**: How would you use SHAP values to explain a model decision to a non-technical stakeholder?

3. **Explore**: When might SHAP values differ from traditional feature importance? What does each measure?
