# Named Entity Recognition

## Introduction

Named Entity Recognition (NER) identifies and classifies named entities in text into predefined categories like persons, organizations, locations, dates, and more.

## What is NER?

```python
import numpy as np
import pandas as pd
import re

print("=== NAMED ENTITY RECOGNITION ===")
print("""
NER: Find and classify named entities in text

Common Entity Types:
  - PERSON: Names of people
  - ORG: Organizations, companies
  - GPE/LOC: Countries, cities, locations
  - DATE: Dates, time expressions
  - MONEY: Monetary values
  - PRODUCT: Products, items

Example:
  "Apple Inc. was founded by Steve Jobs in Cupertino in 1976."
  
  Apple Inc. → ORG
  Steve Jobs → PERSON
  Cupertino → GPE
  1976 → DATE
  
Applications:
  - Information extraction
  - Question answering
  - Knowledge graph construction
  - Search engines
""")
```

## Rule-Based NER

```python
print("\n=== RULE-BASED NER ===")
print("""
Simple approach using patterns and rules:
  - Regular expressions
  - Keyword lists
  - Capitalization rules
  
Pros: No training data needed, interpretable
Cons: Limited coverage, brittle
""")

def rule_based_ner(text):
    """Simple rule-based NER"""
    entities = []
    
    # Date patterns
    date_pattern = r'\b\d{4}\b|\b\d{1,2}/\d{1,2}/\d{2,4}\b'
    for match in re.finditer(date_pattern, text):
        entities.append((match.group(), 'DATE', match.span()))
    
    # Money patterns
    money_pattern = r'\$[\d,]+\.?\d*|\d+\s*(?:dollars|USD)'
    for match in re.finditer(money_pattern, text, re.IGNORECASE):
        entities.append((match.group(), 'MONEY', match.span()))
    
    # Email patterns
    email_pattern = r'\b[\w.-]+@[\w.-]+\.\w+\b'
    for match in re.finditer(email_pattern, text):
        entities.append((match.group(), 'EMAIL', match.span()))
    
    # Capitalized sequences (potential names/orgs)
    cap_pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b'
    for match in re.finditer(cap_pattern, text):
        entities.append((match.group(), 'POTENTIAL_NAME', match.span()))
    
    return entities

text = "John Smith works at Microsoft since 2020. Contact him at john@email.com for $500."
entities = rule_based_ner(text)

print(f"Text: '{text}'")
print("\nEntities found:")
for entity, label, span in entities:
    print(f"  {entity} → {label}")
```

## IOB Tagging

```python
print("\n=== IOB TAGGING ===")
print("""
NER as sequence labeling with IOB format:

B-XXX: Beginning of entity type XXX
I-XXX: Inside entity type XXX  
O: Outside (not an entity)

Example:
  "Barack Obama visited New York City"
  
  Barack → B-PERSON
  Obama  → I-PERSON
  visited → O
  New    → B-LOC
  York   → I-LOC
  City   → I-LOC
  
This format handles:
  - Multi-word entities
  - Consecutive entities
""")

def create_iob_tags(text, entity_spans):
    """Convert text and entity spans to IOB format"""
    words = text.split()
    tags = ['O'] * len(words)
    
    # Simple word position tracking
    pos = 0
    word_positions = []
    for word in words:
        start = text.find(word, pos)
        end = start + len(word)
        word_positions.append((start, end))
        pos = end
    
    # Assign tags
    for entity_text, entity_type, (ent_start, ent_end) in entity_spans:
        first_word = True
        for i, (w_start, w_end) in enumerate(word_positions):
            if w_start >= ent_start and w_end <= ent_end:
                if first_word:
                    tags[i] = f'B-{entity_type}'
                    first_word = False
                else:
                    tags[i] = f'I-{entity_type}'
    
    return list(zip(words, tags))

# Example
text = "Barack Obama visited New York"
entities = [("Barack Obama", "PERSON", (0, 12)), ("New York", "LOC", (21, 29))]

iob_tagged = create_iob_tags(text, entities)
print("IOB Tagged:")
for word, tag in iob_tagged:
    print(f"  {word:<10} {tag}")
```

## Feature-Based NER

```python
print("\n=== FEATURE-BASED NER ===")
print("""
Traditional ML approach with hand-crafted features:

Word features:
  - Is capitalized?
  - Is all caps?
  - Contains digits?
  - Word length
  - Word prefix/suffix
  
Context features:
  - Previous/next word
  - Previous/next POS tag
  - Position in sentence
  
Used with:
  - CRF (Conditional Random Fields)
  - HMM (Hidden Markov Models)
  - Maximum Entropy classifiers
""")

def extract_word_features(word, position, sentence):
    """Extract features for a word"""
    features = {
        'word': word.lower(),
        'is_capitalized': word[0].isupper(),
        'is_all_caps': word.isupper(),
        'is_all_lower': word.islower(),
        'has_digit': any(c.isdigit() for c in word),
        'is_digit': word.isdigit(),
        'length': len(word),
        'prefix_2': word[:2].lower(),
        'suffix_2': word[-2:].lower(),
        'prefix_3': word[:3].lower(),
        'suffix_3': word[-3:].lower(),
        'position': position,
    }
    
    # Context features
    if position > 0:
        features['prev_word'] = sentence[position - 1].lower()
    else:
        features['prev_word'] = '<START>'
        
    if position < len(sentence) - 1:
        features['next_word'] = sentence[position + 1].lower()
    else:
        features['next_word'] = '<END>'
    
    return features

sentence = ["Barack", "Obama", "visited", "Paris"]
print("Features for each word:")
for i, word in enumerate(sentence):
    features = extract_word_features(word, i, sentence)
    print(f"\n  {word}:")
    for k, v in list(features.items())[:6]:
        print(f"    {k}: {v}")
```

## Using SpaCy for NER

```python
print("\n=== SPACY NER ===")
print("""
SpaCy provides pre-trained NER models:
  - English: en_core_web_sm, en_core_web_lg
  - Multiple languages available
  - Fast and accurate

Entity types in spaCy:
  PERSON, ORG, GPE, LOC, DATE, TIME, 
  MONEY, PERCENT, PRODUCT, EVENT, etc.
""")

# Simulated SpaCy-like output
def simulate_spacy_ner(text):
    """Simulate SpaCy NER output"""
    # Pre-defined entities (normally SpaCy would detect these)
    known_entities = {
        'Apple': ('Apple', 'ORG'),
        'Apple Inc': ('Apple Inc', 'ORG'),
        'Google': ('Google', 'ORG'),
        'Microsoft': ('Microsoft', 'ORG'),
        'New York': ('New York', 'GPE'),
        'Paris': ('Paris', 'GPE'),
        'London': ('London', 'GPE'),
        'Steve Jobs': ('Steve Jobs', 'PERSON'),
        'Barack Obama': ('Barack Obama', 'PERSON'),
        'Elon Musk': ('Elon Musk', 'PERSON'),
    }
    
    entities = []
    text_lower = text.lower()
    
    for key, (entity_text, entity_type) in known_entities.items():
        if key.lower() in text_lower:
            # Find position
            start = text.lower().find(key.lower())
            end = start + len(key)
            entities.append({
                'text': text[start:end],
                'label': entity_type,
                'start': start,
                'end': end
            })
    
    return entities

# Example
text = "Steve Jobs founded Apple in California. Elon Musk runs Tesla."
entities = simulate_spacy_ner(text)

print(f"Text: '{text}'")
print("\nEntities (SpaCy-style):")
for ent in entities:
    print(f"  {ent['text']} ({ent['label']}) [{ent['start']}:{ent['end']}]")

print("""
Actual SpaCy usage:
  import spacy
  nlp = spacy.load('en_core_web_sm')
  doc = nlp(text)
  for ent in doc.ents:
      print(ent.text, ent.label_)
""")
```

## NER Evaluation

```python
print("\n=== NER EVALUATION ===")
print("""
Metrics for NER:

EXACT MATCH:
  - Entity boundaries AND type must match exactly
  
PARTIAL MATCH:
  - Allow boundary variations
  - Count overlapping entities

Metrics:
  - Precision: Correct predictions / All predictions
  - Recall: Correct predictions / All actual entities
  - F1 Score: Harmonic mean of precision and recall
  
Often evaluated per entity type.
""")

def evaluate_ner(pred_entities, true_entities):
    """Evaluate NER predictions"""
    # Convert to sets of (text, type) tuples
    pred_set = set((e['text'].lower(), e['label']) for e in pred_entities)
    true_set = set((e['text'].lower(), e['label']) for e in true_entities)
    
    correct = pred_set & true_set
    
    precision = len(correct) / len(pred_set) if pred_set else 0
    recall = len(correct) / len(true_set) if true_set else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {'precision': precision, 'recall': recall, 'f1': f1}

# Example
pred = [{'text': 'Apple', 'label': 'ORG'}, {'text': 'Steve Jobs', 'label': 'PERSON'}]
true = [{'text': 'Apple', 'label': 'ORG'}, {'text': 'Steve Jobs', 'label': 'PERSON'}, 
        {'text': 'California', 'label': 'GPE'}]

metrics = evaluate_ner(pred, true)
print("Evaluation:")
print(f"  Precision: {metrics['precision']:.2f}")
print(f"  Recall: {metrics['recall']:.2f}")
print(f"  F1: {metrics['f1']:.2f}")
```

## Common Challenges

```python
print("\n=== COMMON CHALLENGES ===")
print("""
1. AMBIGUITY:
   - "Apple" → Company or fruit?
   - "Washington" → Person, city, or state?
   Context matters!

2. NESTED ENTITIES:
   - "Bank of America headquarters"
   - "Bank of America" is ORG inside larger phrase

3. RARE/NEW ENTITIES:
   - New company names
   - Emerging terms
   - Out-of-vocabulary words

4. DOMAIN ADAPTATION:
   - Medical entities differ from news
   - Legal terminology
   - Technical jargon

5. MULTI-WORD ENTITIES:
   - "New York City"
   - "United States of America"
   - Boundary detection is tricky
""")

# Ambiguity example
ambiguous_texts = [
    "I bought an apple at the store.",  # fruit
    "I bought Apple stock yesterday.",  # company
]

print("Ambiguity examples:")
for text in ambiguous_texts:
    print(f"  '{text}'")
```

## Key Points

- **NER**: Identifies named entities (persons, orgs, locations, etc.)
- **IOB tagging**: Standard format for sequence labeling
- **Rule-based**: Patterns and gazetteers, no training needed
- **Feature-based**: Hand-crafted features with CRF/HMM
- **Deep learning**: BiLSTM-CRF, Transformers
- **Evaluation**: Precision, recall, F1 (often per entity type)
- **Challenges**: Ambiguity, nested entities, domain adaptation

## Reflection Questions

1. Why is context important for resolving entity ambiguity?
2. How would you adapt a general NER model for a specialized domain like medicine?
3. What are the trade-offs between rule-based and ML-based NER approaches?
