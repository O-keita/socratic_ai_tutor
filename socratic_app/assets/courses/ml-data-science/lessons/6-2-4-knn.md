# K-Nearest Neighbors

## Introduction

K-Nearest Neighbors (KNN) is a simple, non-parametric algorithm that makes predictions based on the similarity to training examples. It's intuitive, requires no training phase, but can be computationally expensive for large datasets.

## The KNN Algorithm

```python
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

np.random.seed(42)

print("=== K-NEAREST NEIGHBORS ===")
print("""
Algorithm:
  1. Store all training data
  2. For a new point:
     a. Calculate distance to ALL training points
     b. Find K nearest neighbors
     c. Classification: Majority vote
        Regression: Average of neighbors' values

Key Idea: "You are the average of your neighbors"

Properties:
  - Instance-based (lazy) learning
  - No explicit training phase
  - Non-parametric (no assumptions about data)
  - Simple and intuitive
""")
```

## KNN Classification

```python
print("\n=== KNN CLASSIFICATION ===")

# Load iris dataset
iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features (important for KNN!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)

accuracy = knn.score(X_test_scaled, y_test)
print(f"KNN (K=5) Accuracy: {accuracy:.3f}")

# Predict with probabilities
sample = X_test_scaled[0:1]
prediction = knn.predict(sample)
probabilities = knn.predict_proba(sample)

print(f"\nSample prediction:")
print(f"  Predicted class: {prediction[0]}")
print(f"  Probabilities: {probabilities[0].round(3)}")
print(f"  Actual class: {y_test[0]}")
```

## Choosing K

```python
print("\n=== CHOOSING K ===")
print("""
K is the most important hyperparameter:

Small K (e.g., K=1):
  - Model is very flexible
  - Sensitive to noise and outliers
  - High variance, low bias
  - Risk of overfitting

Large K:
  - Model is more smooth
  - Less sensitive to noise
  - High bias, low variance
  - Risk of underfitting

Guidelines:
  - Often √n (square root of training size)
  - Use cross-validation to tune
  - Odd K avoids ties in binary classification
""")

# Test different values of K
k_values = [1, 3, 5, 7, 9, 11, 15, 21]
print("Accuracy for different K values:")
print(f"{'K':>4} | Train Acc | Test Acc")
print("-" * 30)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    train_acc = knn.score(X_train_scaled, y_train)
    test_acc = knn.score(X_test_scaled, y_test)
    print(f"{k:>4} | {train_acc:.3f}     | {test_acc:.3f}")

# Use cross-validation to find best K
print("\n5-Fold CV scores:")
for k in [3, 5, 7, 9]:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)
    print(f"  K={k}: {scores.mean():.3f} ± {scores.std():.3f}")
```

## Distance Metrics

```python
print("\n=== DISTANCE METRICS ===")
print("""
KNN depends on distance calculation:

EUCLIDEAN (default):
  d = √Σ(xᵢ - yᵢ)²
  - Standard "straight line" distance
  - Works well for continuous features

MANHATTAN (L1):
  d = Σ|xᵢ - yᵢ|
  - Sum of absolute differences
  - More robust to outliers

MINKOWSKI:
  d = (Σ|xᵢ - yᵢ|ᵖ)^(1/p)
  - Generalization (p=2 is Euclidean, p=1 is Manhattan)

COSINE:
  d = 1 - (x·y)/(|x||y|)
  - Based on angle between vectors
  - Good for text/high-dimensional sparse data
""")

# Compare distance metrics
from sklearn.neighbors import KNeighborsClassifier

metrics = ['euclidean', 'manhattan', 'chebyshev']
print("Accuracy with different distance metrics:")

for metric in metrics:
    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)
    knn.fit(X_train_scaled, y_train)
    acc = knn.score(X_test_scaled, y_test)
    print(f"  {metric}: {acc:.3f}")
```

## The Importance of Scaling

```python
print("\n=== IMPORTANCE OF SCALING ===")
print("""
KNN uses distance, so feature scales MATTER!

Example without scaling:
  Feature A: Range 0-1
  Feature B: Range 0-1000
  
  Feature B will dominate distance calculations!
  
ALWAYS scale features for KNN:
  - StandardScaler: z = (x - mean) / std
  - MinMaxScaler: scale to [0, 1]
""")

# Demonstrate scaling importance
print("Accuracy with vs without scaling:")

# Without scaling
knn_unscaled = KNeighborsClassifier(n_neighbors=5)
knn_unscaled.fit(X_train, y_train)
acc_unscaled = knn_unscaled.score(X_test, y_test)
print(f"  Without scaling: {acc_unscaled:.3f}")

# With scaling
knn_scaled = KNeighborsClassifier(n_neighbors=5)
knn_scaled.fit(X_train_scaled, y_train)
acc_scaled = knn_scaled.score(X_test_scaled, y_test)
print(f"  With scaling: {acc_scaled:.3f}")

print(f"\nFeature ranges (unscaled):")
for i, name in enumerate(iris.feature_names):
    print(f"  {name}: [{X[:, i].min():.1f}, {X[:, i].max():.1f}]")
```

## KNN for Regression

```python
print("\n=== KNN REGRESSION ===")

# Generate regression data
np.random.seed(42)
X_reg = np.sort(np.random.uniform(0, 10, 100)).reshape(-1, 1)
y_reg = np.sin(X_reg.flatten()) + np.random.normal(0, 0.2, 100)

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

print("KNN Regression performance:")
for k in [1, 3, 5, 10, 20]:
    knn_reg = KNeighborsRegressor(n_neighbors=k)
    knn_reg.fit(X_train_r, y_train_r)
    r2 = knn_reg.score(X_test_r, y_test_r)
    print(f"  K={k:2d}: R² = {r2:.3f}")

# Weighted voting
print("\nWith distance weighting:")
knn_weighted = KNeighborsRegressor(n_neighbors=5, weights='distance')
knn_weighted.fit(X_train_r, y_train_r)
r2_weighted = knn_weighted.score(X_test_r, y_test_r)
print(f"  K=5 (weighted): R² = {r2_weighted:.3f}")
```

## Weighted Voting

```python
print("\n=== WEIGHTED VOTING ===")
print("""
Standard KNN: All neighbors vote equally
Weighted KNN: Closer neighbors have more influence

Weight options:
  'uniform': All equal weight (default)
  'distance': Weight = 1/distance
  
Weighted often performs better, especially with large K.
""")

# Compare uniform vs distance weighting
print("Comparison (Iris classification):")
for weights in ['uniform', 'distance']:
    knn = KNeighborsClassifier(n_neighbors=10, weights=weights)
    knn.fit(X_train_scaled, y_train)
    acc = knn.score(X_test_scaled, y_test)
    print(f"  weights='{weights}': {acc:.3f}")
```

## Advantages and Disadvantages

```python
print("\n=== PROS AND CONS ===")
print("""
ADVANTAGES:
  ✓ Simple and intuitive
  ✓ No training phase (lazy learning)
  ✓ Naturally handles multi-class
  ✓ No assumptions about data distribution
  ✓ Can adapt to new data easily
  ✓ Works well with small datasets

DISADVANTAGES:
  ✗ Computationally expensive at prediction time
  ✗ Requires lots of memory (stores all data)
  ✗ Sensitive to feature scales
  ✗ Struggles with high dimensions (curse of dimensionality)
  ✗ Sensitive to irrelevant features
  ✗ Imbalanced classes problematic

WHEN TO USE KNN:
  - Small to medium datasets
  - Quick baseline model
  - Low-dimensional data
  - Non-linear decision boundaries needed
  
WHEN NOT TO USE:
  - Large datasets (prediction too slow)
  - High-dimensional data (distances become meaningless)
  - Real-time predictions needed
""")
```

## Key Points

- **Algorithm**: Predict based on K nearest training examples
- **K parameter**: Controls model complexity (bias-variance tradeoff)
- **Distance metric**: Euclidean, Manhattan, or others
- **Scaling required**: Features must be on same scale
- **Weighted voting**: Closer neighbors can have more influence
- **Lazy learning**: No training, all computation at prediction
- **Works for both**: Classification and regression

## Reflection Questions

1. Why does KNN become less effective as dimensionality increases?
2. How would you handle imbalanced classes with KNN?
3. When might Manhattan distance be preferred over Euclidean?
