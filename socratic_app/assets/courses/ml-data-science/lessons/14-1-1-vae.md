# Variational Autoencoders (VAEs)

## Introduction

Variational Autoencoders combine neural network autoencoders with Bayesian inference to learn latent representations and generate new data.

## From Autoencoders to VAEs

```python
import numpy as np
import pandas as pd

print("=== AUTOENCODERS REVIEW ===")
print("""
STANDARD AUTOENCODER:
  Encoder: x → z (compressed representation)
  Decoder: z → x' (reconstruction)
  Loss: ||x - x'||² (reconstruction error)

Problem: Latent space not smooth
  - Points clustered arbitrarily
  - Gaps in latent space
  - Can't sample meaningful new points

Solution: VAE - learn a PROBABILITY DISTRIBUTION
""")

print("""
VAE KEY IDEA:
  Instead of z = encode(x)
  Learn z ~ N(μ(x), σ(x))
  
  Encoder outputs:
    - μ (mean vector)
    - σ (standard deviation vector)
  
  Sample z from this distribution:
    z = μ + σ ⊙ ε, where ε ~ N(0, I)
  
  Benefits:
    - Smooth latent space
    - Can sample anywhere
    - Meaningful interpolation
""")
```

## VAE Architecture

```python
print("\n=== VAE ARCHITECTURE ===")
print("""
        Input x
           ↓
    ┌──────────────┐
    │    Encoder   │
    │   (neural    │
    │    network)  │
    └───────┬──────┘
            ↓
    ┌───────┴───────┐
    ↓               ↓
   [μ]             [log σ²]
    ↓               ↓
    └───────┬───────┘
            ↓
    Reparameterization
    z = μ + σ ⊙ ε
            ↓
    ┌──────────────┐
    │   Decoder    │
    │   (neural    │
    │    network)  │
    └───────┬──────┘
            ↓
    Reconstruction x'
    
Note: We use log σ² instead of σ
  - More numerically stable
  - Can be any real number
  - σ = exp(0.5 × log σ²)
""")
```

## The Reparameterization Trick

```python
print("\n=== REPARAMETERIZATION TRICK ===")
print("""
PROBLEM: Can't backpropagate through random sampling

    μ, σ → sample z ~ N(μ, σ) → decoder → loss
                 ↑
           Can't differentiate!

SOLUTION: Reparameterize the sampling

    ε ~ N(0, I)           (random noise, external)
    z = μ + σ ⊙ ε         (deterministic given μ, σ, ε)
    
Now gradients flow through μ and σ!
    loss → decoder → z → μ, σ → encoder
                      ↘ ε (no gradient needed)
""")

def sample_z(mu, log_var):
    """Reparameterization trick"""
    # Standard deviation from log variance
    std = np.exp(0.5 * log_var)
    
    # Random noise
    epsilon = np.random.randn(*mu.shape)
    
    # Sample z
    z = mu + std * epsilon
    
    return z

# Example
mu = np.array([0.5, -0.3, 0.8])
log_var = np.array([-1.0, 0.0, 0.5])

print("Sampling with reparameterization:")
print(f"μ = {mu}")
print(f"log σ² = {log_var}")
print(f"σ = {np.exp(0.5 * log_var).round(3)}")

samples = [sample_z(mu, log_var) for _ in range(3)]
for i, s in enumerate(samples):
    print(f"Sample {i+1}: {s.round(3)}")
```

## VAE Loss Function

```python
print("\n=== VAE LOSS (ELBO) ===")
print("""
Loss = Reconstruction Loss + KL Divergence

1. RECONSTRUCTION LOSS:
   How well does decoder reconstruct input?
   - Binary: Binary cross-entropy
   - Continuous: Mean squared error
   
   L_recon = ||x - decoder(z)||²

2. KL DIVERGENCE:
   How close is q(z|x) to prior p(z) = N(0, I)?
   
   KL(q(z|x) || p(z)) = -0.5 × Σ(1 + log σ² - μ² - σ²)
   
   Regularizes latent space toward standard normal.

Total loss (negative ELBO):
   L = L_recon + β × KL
   
   β = 1: Standard VAE
   β > 1: β-VAE (more disentanglement)
""")

def vae_loss(x, x_recon, mu, log_var, beta=1.0):
    """VAE loss function"""
    # Reconstruction loss (MSE)
    recon_loss = np.mean((x - x_recon) ** 2)
    
    # KL divergence
    kl_loss = -0.5 * np.mean(1 + log_var - mu**2 - np.exp(log_var))
    
    total_loss = recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss

# Example
x = np.array([1.0, 0.5, 0.8])
x_recon = np.array([0.9, 0.6, 0.75])
mu = np.array([0.1, -0.2, 0.3])
log_var = np.array([-0.5, 0.0, -0.3])

total, recon, kl = vae_loss(x, x_recon, mu, log_var)
print(f"Reconstruction loss: {recon:.4f}")
print(f"KL divergence: {kl:.4f}")
print(f"Total VAE loss: {total:.4f}")
```

## Keras Implementation

```python
print("\n=== VAE IN KERAS ===")
print("""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class Sampling(layers.Layer):
    '''Reparameterization trick layer'''
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.random.normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# Encoder
latent_dim = 2

encoder_inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)
x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)
x = layers.Flatten()(x)
x = layers.Dense(16, activation='relu')(x)
z_mean = layers.Dense(latent_dim, name='z_mean')(x)
z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)
z = Sampling()([z_mean, z_log_var])

encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')

# Decoder
latent_inputs = keras.Input(shape=(latent_dim,))
x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)
x = layers.Reshape((7, 7, 64))(x)
x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)
x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)
decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)

decoder = keras.Model(latent_inputs, decoder_outputs, name='decoder')
""")
```

## Latent Space Properties

```python
print("\n=== LATENT SPACE PROPERTIES ===")
print("""
VAE latent space is:

1. CONTINUOUS
   - No gaps between points
   - Can sample anywhere
   
2. SMOOTH
   - Small movements → small output changes
   - Nearby points = similar samples
   
3. INTERPOLATABLE
   - Linear interpolation in z-space
   - Meaningful transitions between samples
   
   z_interp = α × z₁ + (1-α) × z₂
   
4. ORGANIZED
   - Similar inputs cluster together
   - Structure reflects data structure
""")

def interpolate_latent(z1, z2, steps=5):
    """Linear interpolation in latent space"""
    alphas = np.linspace(0, 1, steps)
    interpolated = []
    
    for alpha in alphas:
        z = alpha * z1 + (1 - alpha) * z2
        interpolated.append(z)
    
    return interpolated

z1 = np.array([1.0, 0.5])
z2 = np.array([-1.0, 0.8])

print("Interpolation from z1 to z2:")
for i, z in enumerate(interpolate_latent(z1, z2)):
    print(f"  Step {i}: {z.round(3)}")
```

## Generation and Manipulation

```python
print("\n=== GENERATING NEW SAMPLES ===")
print("""
To generate new data:

1. RANDOM SAMPLING
   z ~ N(0, I)
   x_new = decoder(z)

2. INTERPOLATION
   z = α × encode(x₁) + (1-α) × encode(x₂)
   x_interp = decoder(z)

3. ARITHMETIC
   z_smiling = encode(smiling_face) - encode(neutral_face)
   z_new = encode(my_face) + z_smiling
   smiling_me = decoder(z_new)

4. CONDITIONAL GENERATION (CVAE)
   z ~ N(0, I)
   x = decoder(z, condition)
   
   Example: Generate digit "7"
   x = decoder(z, label=7)
""")
```

## β-VAE and Disentanglement

```python
print("\n=== β-VAE AND DISENTANGLEMENT ===")
print("""
β-VAE: Increase KL weight

Loss = Recon + β × KL  (β > 1)

Higher β:
  - Stronger regularization
  - Latent dimensions more independent
  - "Disentangled" representations
  
Disentanglement means:
  - Each dimension captures one factor
  - z[0] = rotation
  - z[1] = thickness
  - z[2] = color
  - etc.
  
Trade-off:
  - β ↑: More disentanglement, worse reconstruction
  - β ↓: Better reconstruction, less disentanglement
""")

beta_values = [0.5, 1.0, 4.0, 10.0]
print("Effect of β:")
for beta in beta_values:
    if beta < 1:
        quality = "Better reconstruction, entangled"
    elif beta == 1:
        quality = "Standard VAE"
    elif beta <= 4:
        quality = "Some disentanglement"
    else:
        quality = "Strong disentanglement, blurry"
    print(f"  β = {beta:4.1f}: {quality}")
```

## Key Points

- **VAE**: Learn latent distribution, not just point encoding
- **Reparameterization**: z = μ + σε enables backprop through sampling
- **ELBO loss**: Reconstruction + KL divergence
- **Smooth latent space**: Enables interpolation and sampling
- **β-VAE**: Higher β for more disentanglement
- **Generation**: Sample z ~ N(0,I), decode to get new data

## Reflection Questions

1. Why does the KL term encourage the latent space to be smooth?
2. How does the reparameterization trick enable gradient-based learning?
3. What's the trade-off between reconstruction quality and disentanglement?
