# Gaussian Mixture Models

## Introduction

Gaussian Mixture Models (GMM) assume data is generated from a mixture of several Gaussian distributions. Unlike K-Means, GMM provides soft cluster assignments (probabilities) and can model elliptical clusters.

## What are Gaussian Mixture Models?

```python
import numpy as np
import pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score

np.random.seed(42)

print("=== GAUSSIAN MIXTURE MODELS ===")
print("""
Assumption:
  Data is generated from K Gaussian distributions
  Each data point comes from one of these distributions

Model Parameters (for each cluster k):
  - μₖ (mean): Center of the Gaussian
  - Σₖ (covariance): Shape of the Gaussian
  - πₖ (weight): Probability of belonging to cluster k

Probability of a point:
  P(x) = Σₖ πₖ * N(x | μₖ, Σₖ)

Key Difference from K-Means:
  - K-Means: Hard assignment (each point to one cluster)
  - GMM: Soft assignment (probability for each cluster)
""")
```

## GMM in Practice

```python
print("\n=== GMM IN PRACTICE ===")

# Generate sample data
X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.8, random_state=42)

# Fit GMM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Hard predictions (most likely cluster)
labels = gmm.predict(X)

# Soft predictions (probabilities)
probs = gmm.predict_proba(X)

print("GMM Results:")
print(f"  Number of components: {gmm.n_components}")
print(f"  Converged: {gmm.converged_}")
print(f"  Iterations: {gmm.n_iter_}")

print(f"\nCluster sizes (hard assignment): {np.bincount(labels)}")

print(f"\nSample probability assignments (first 5 points):")
print(f"{'Point':>6} | {'P(C0)':>8} | {'P(C1)':>8} | {'P(C2)':>8} | Assigned")
print("-" * 50)
for i in range(5):
    print(f"{i:>6} | {probs[i,0]:>8.3f} | {probs[i,1]:>8.3f} | {probs[i,2]:>8.3f} | {labels[i]}")
```

## Model Parameters

```python
print("\n=== MODEL PARAMETERS ===")

print("Means (cluster centers):")
for i, mean in enumerate(gmm.means_):
    print(f"  Cluster {i}: [{mean[0]:.2f}, {mean[1]:.2f}]")

print("\nCovariances (cluster shapes):")
for i, cov in enumerate(gmm.covariances_):
    print(f"  Cluster {i}:")
    print(f"    [[{cov[0,0]:.3f}, {cov[0,1]:.3f}]")
    print(f"     [{cov[1,0]:.3f}, {cov[1,1]:.3f}]]")

print("\nWeights (mixing proportions):")
for i, weight in enumerate(gmm.weights_):
    print(f"  Cluster {i}: {weight:.3f}")

print(f"\nWeights sum to: {gmm.weights_.sum():.3f}")
```

## Covariance Types

```python
print("\n=== COVARIANCE TYPES ===")
print("""
GMM can use different covariance constraints:

'full' (default):
  - Each cluster has its own full covariance matrix
  - Most flexible, can model any ellipse orientation
  - Most parameters

'tied':
  - All clusters share the same covariance matrix
  - Different centers, same shape
  - Fewer parameters

'diag':
  - Diagonal covariance (no correlation between features)
  - Ellipses aligned with axes
  - Fewer parameters

'spherical':
  - Spherical clusters (like K-Means)
  - Single variance per cluster
  - Fewest parameters
""")

cov_types = ['full', 'tied', 'diag', 'spherical']

print("Comparing covariance types:")
for cov_type in cov_types:
    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)
    gmm.fit(X)
    labels = gmm.predict(X)
    score = silhouette_score(X, labels)
    print(f"  {cov_type:10s}: silhouette={score:.3f}, BIC={gmm.bic(X):.1f}")
```

## The EM Algorithm

```python
print("\n=== THE EM ALGORITHM ===")
print("""
GMM is trained using Expectation-Maximization (EM):

E-STEP (Expectation):
  - Compute probability each point belongs to each cluster
  - Uses current parameters
  - "Soft" assignments

M-STEP (Maximization):
  - Update parameters to maximize likelihood
  - Use weighted points (by responsibilities)
  - Update means, covariances, weights

Repeat E-step and M-step until convergence

Similar to K-Means:
  K-Means E-step: Hard assignment (nearest centroid)
  K-Means M-step: Update centroids (mean of assigned points)
  
GMM is a "soft" version of K-Means!
""")

# Show EM iterations
print("Training progress (log-likelihood):")
gmm = GaussianMixture(n_components=3, max_iter=1, warm_start=True, random_state=42)

for i in range(10):
    gmm.fit(X)
    ll = gmm.score(X) * len(X)  # Total log-likelihood
    print(f"  Iteration {i+1}: Log-likelihood = {ll:.2f}")
```

## Choosing Number of Components

```python
print("\n=== CHOOSING NUMBER OF COMPONENTS ===")
print("""
Model selection criteria:

AIC (Akaike Information Criterion):
  AIC = -2 * log-likelihood + 2 * n_parameters
  - Penalizes complexity
  - Lower is better

BIC (Bayesian Information Criterion):
  BIC = -2 * log-likelihood + log(n) * n_parameters
  - Stronger penalty than AIC
  - Lower is better
  - Often preferred

Choose K that minimizes AIC or BIC.
""")

print("Model selection for different K:")
print(f"{'K':>3} | {'AIC':>10} | {'BIC':>10} | {'Log-Lik':>12}")
print("-" * 45)

for k in range(1, 7):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    print(f"{k:>3} | {gmm.aic(X):>10.1f} | {gmm.bic(X):>10.1f} | {gmm.score(X)*len(X):>12.1f}")

print("\nBoth AIC and BIC suggest K=3 (we generated 3 clusters!)")
```

## GMM vs K-Means

```python
print("\n=== GMM vs K-MEANS ===")
print("""
                        | K-MEANS           | GMM
------------------------+-------------------+--------------------
Cluster shape           | Spherical only    | Any ellipse
Assignment              | Hard (0/1)        | Soft (probabilities)
Model                   | Distance-based    | Probabilistic
Covariance              | Implicit (equal)  | Explicit (flexible)
Initialization          | Sensitive         | Less sensitive
Interpretation          | Centroids         | Distributions
Speed                   | Faster            | Slower
Use case                | General           | When shapes vary
""")

from sklearn.cluster import KMeans

# Generate elliptical clusters
np.random.seed(42)
# Cluster 1: horizontal ellipse
c1 = np.random.randn(100, 2) @ np.array([[3, 0], [0, 0.5]]) + [0, 0]
# Cluster 2: vertical ellipse
c2 = np.random.randn(100, 2) @ np.array([[0.5, 0], [0, 3]]) + [5, 0]
X_ellipse = np.vstack([c1, c2])
y_ellipse = np.array([0]*100 + [1]*100)

# Compare
km = KMeans(n_clusters=2, random_state=42, n_init=10)
gmm = GaussianMixture(n_components=2, random_state=42)

labels_km = km.fit_predict(X_ellipse)
labels_gmm = gmm.fit_predict(X_ellipse)

from sklearn.metrics import adjusted_rand_score
print("Elliptical clusters comparison:")
print(f"  K-Means ARI: {adjusted_rand_score(y_ellipse, labels_km):.3f}")
print(f"  GMM ARI: {adjusted_rand_score(y_ellipse, labels_gmm):.3f}")
print("\nGMM handles elliptical shapes better!")
```

## Density Estimation and Sampling

```python
print("\n=== DENSITY ESTIMATION AND SAMPLING ===")
print("""
GMM is also a GENERATIVE MODEL:
  - Can estimate probability density p(x)
  - Can generate new samples from the learned distribution
""")

# Fit GMM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Score samples (log-likelihood)
log_probs = gmm.score_samples(X[:5])
print("Log-probability of first 5 points:")
for i, lp in enumerate(log_probs):
    print(f"  Point {i}: log P(x) = {lp:.3f}")

# Generate new samples
X_new, component_labels = gmm.sample(10)
print(f"\n10 new samples generated from GMM:")
print(f"  Shape: {X_new.shape}")
print(f"  Components: {component_labels.flatten()}")

# Anomaly detection
print("\nAnomaly detection (low probability = anomaly):")
outlier = np.array([[10, 10]])  # Far from clusters
normal = np.array([[gmm.means_[0]]])  # At cluster center
print(f"  Normal point log-prob: {gmm.score_samples(normal)[0]:.3f}")
print(f"  Outlier point log-prob: {gmm.score_samples(outlier)[0]:.3f}")
```

## Key Points

- **GMM**: Probabilistic model assuming Gaussian mixture
- **Soft clustering**: Probability of belonging to each cluster
- **EM algorithm**: Iteratively estimate and maximize
- **Covariance types**: full, tied, diagonal, spherical
- **Model selection**: Use BIC or AIC to choose K
- **Generative model**: Can sample new data points
- **Better shapes**: Handles elliptical clusters unlike K-Means

## Reflection Questions

1. When would soft cluster assignments be more useful than hard assignments?
2. How does the covariance type affect the number of parameters GMM needs to learn?
3. Why might GMM be preferred for anomaly detection over K-Means?
