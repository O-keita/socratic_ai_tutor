# Support Vector Machines

## Introduction

Support Vector Machines (SVM) find the optimal hyperplane that separates classes with maximum margin. They're powerful for both linear and non-linear classification, with strong theoretical foundations.

## The Maximum Margin Classifier

```python
import numpy as np
import pandas as pd
from sklearn.svm import SVC, SVR
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification, make_moons

np.random.seed(42)

print("=== SUPPORT VECTOR MACHINES ===")
print("""
Core Idea: Find the hyperplane that:
  1. Separates the classes
  2. Maximizes the MARGIN (distance to nearest points)

Key Concepts:
  - Hyperplane: Decision boundary (line in 2D, plane in 3D)
  - Margin: Distance between hyperplane and closest points
  - Support Vectors: The closest points that define the margin

Why maximize margin?
  - Better generalization
  - More robust to new data
  - Unique optimal solution
""")
```

## Linear SVM

```python
print("\n=== LINEAR SVM ===")

# Generate linearly separable data
X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,
                          n_informative=2, n_clusters_per_class=1,
                          class_sep=2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit linear SVM
svm_linear = SVC(kernel='linear')
svm_linear.fit(X_train_scaled, y_train)

print(f"Linear SVM Accuracy: {svm_linear.score(X_test_scaled, y_test):.3f}")

# Support vectors
print(f"\nSupport Vectors:")
print(f"  Number of SVs: {len(svm_linear.support_)}")
print(f"  SVs per class: {svm_linear.n_support_}")
print(f"  Total training points: {len(X_train)}")
print(f"  Fraction that are SVs: {len(svm_linear.support_)/len(X_train):.2%}")
```

## The C Parameter (Regularization)

```python
print("\n=== THE C PARAMETER ===")
print("""
C controls the tradeoff between:
  - Maximizing margin (simpler model)
  - Minimizing classification errors (fitting training data)

Large C:
  - Smaller margin
  - Fewer margin violations allowed
  - Risk of overfitting

Small C:
  - Larger margin
  - More margin violations allowed
  - Risk of underfitting

Think of C as "cost of misclassification"
""")

# Test different C values
c_values = [0.001, 0.01, 0.1, 1, 10, 100]
print("Effect of C parameter:")
print(f"{'C':>7} | Train Acc | Test Acc | # Support Vectors")
print("-" * 50)

for c in c_values:
    svm = SVC(kernel='linear', C=c)
    svm.fit(X_train_scaled, y_train)
    train_acc = svm.score(X_train_scaled, y_train)
    test_acc = svm.score(X_test_scaled, y_test)
    n_sv = len(svm.support_)
    print(f"{c:>7} |   {train_acc:.3f}   |  {test_acc:.3f}   | {n_sv}")
```

## The Kernel Trick

```python
print("\n=== THE KERNEL TRICK ===")
print("""
Problem: Data isn't always linearly separable
Solution: Map to higher-dimensional space where it IS separable

Kernel Trick:
  - Don't explicitly compute high-dimensional features
  - Use kernel function K(x, x') to compute dot products
  - Much more efficient!

Common Kernels:
  - Linear: K(x, x') = x · x'
  - Polynomial: K(x, x') = (γx · x' + r)^d
  - RBF (Gaussian): K(x, x') = exp(-γ||x - x'||²)
  - Sigmoid: K(x, x') = tanh(γx · x' + r)
""")

# Create non-linear data
X_moon, y_moon = make_moons(n_samples=200, noise=0.15, random_state=42)
X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(
    X_moon, y_moon, test_size=0.2, random_state=42
)

scaler_m = StandardScaler()
X_train_m_scaled = scaler_m.fit_transform(X_train_m)
X_test_m_scaled = scaler_m.transform(X_test_m)

print("Non-linear data (moon-shaped):")
print("Comparing kernels:")

kernels = ['linear', 'poly', 'rbf', 'sigmoid']
for kernel in kernels:
    svm = SVC(kernel=kernel, gamma='auto')
    svm.fit(X_train_m_scaled, y_train_m)
    acc = svm.score(X_test_m_scaled, y_test_m)
    print(f"  {kernel:>8}: {acc:.3f}")
```

## RBF Kernel and Gamma

```python
print("\n=== RBF KERNEL AND GAMMA ===")
print("""
RBF (Radial Basis Function) is most common non-linear kernel:
  K(x, x') = exp(-γ||x - x'||²)

γ (gamma) controls the "reach" of each training example:

Large gamma:
  - Each example has small influence radius
  - More complex boundary (can overfit)
  - "High frequency" decision boundary

Small gamma:
  - Each example influences larger region
  - Smoother boundary (may underfit)
  - "Low frequency" decision boundary

Rule of thumb: Start with gamma = 1 / n_features
""")

# Test different gamma values
gamma_values = [0.01, 0.1, 1, 10, 100]
print("Effect of gamma (RBF kernel):")
print(f"{'gamma':>7} | Train Acc | Test Acc")
print("-" * 35)

for gamma in gamma_values:
    svm = SVC(kernel='rbf', gamma=gamma)
    svm.fit(X_train_m_scaled, y_train_m)
    train_acc = svm.score(X_train_m_scaled, y_train_m)
    test_acc = svm.score(X_test_m_scaled, y_test_m)
    print(f"{gamma:>7} |   {train_acc:.3f}   |  {test_acc:.3f}")
```

## Hyperparameter Tuning

```python
print("\n=== HYPERPARAMETER TUNING ===")
print("""
Key hyperparameters:
  - kernel: Type of kernel function
  - C: Regularization (margin vs. errors)
  - gamma: Kernel coefficient (RBF, poly, sigmoid)
  - degree: Polynomial degree (poly kernel)

Typical approach:
  1. Use RBF kernel (usually works well)
  2. Grid search over C and gamma
  3. Use logarithmic scale for both
""")

from sklearn.model_selection import GridSearchCV

# Grid search
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [0.01, 0.1, 1]
}

grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_m_scaled, y_train_m)

print("Grid Search Results:")
print(f"  Best parameters: {grid_search.best_params_}")
print(f"  Best CV score: {grid_search.best_score_:.3f}")

# Test on held-out data
best_model = grid_search.best_estimator_
test_acc = best_model.score(X_test_m_scaled, y_test_m)
print(f"  Test accuracy: {test_acc:.3f}")
```

## Probability Estimates

```python
print("\n=== PROBABILITY ESTIMATES ===")
print("""
By default, SVM doesn't give probabilities.
Set probability=True to enable (but slower training).

Uses Platt scaling: Fits logistic regression on SVM scores.
""")

svm_prob = SVC(kernel='rbf', probability=True)
svm_prob.fit(X_train_m_scaled, y_train_m)

# Get probabilities
sample = X_test_m_scaled[:3]
predictions = svm_prob.predict(sample)
probabilities = svm_prob.predict_proba(sample)

print("Sample predictions with probabilities:")
for i in range(3):
    print(f"  Sample {i}: Class {predictions[i]}, "
          f"P(0)={probabilities[i,0]:.3f}, P(1)={probabilities[i,1]:.3f}")
```

## SVM for Regression (SVR)

```python
print("\n=== SUPPORT VECTOR REGRESSION (SVR) ===")
print("""
SVR finds a function that:
  - Deviates from targets by at most ε (epsilon)
  - Is as flat as possible (small coefficients)

Parameters:
  - epsilon: Margin of tolerance (no penalty inside ε-tube)
  - C: Regularization (same as classification)
  - kernel: Same options as SVC
""")

# Generate regression data
np.random.seed(42)
X_reg = np.sort(5 * np.random.rand(100, 1), axis=0)
y_reg = np.sin(X_reg.flatten()) + np.random.randn(100) * 0.2

X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

# Compare kernels
print("SVR with different kernels:")
for kernel in ['linear', 'rbf']:
    svr = SVR(kernel=kernel)
    svr.fit(X_train_r, y_train_r)
    r2 = svr.score(X_test_r, y_test_r)
    print(f"  {kernel}: R² = {r2:.3f}")
```

## Advantages and Disadvantages

```python
print("\n=== PROS AND CONS ===")
print("""
ADVANTAGES:
  ✓ Effective in high-dimensional spaces
  ✓ Memory efficient (only stores support vectors)
  ✓ Versatile (different kernels for different data)
  ✓ Good generalization (maximum margin principle)
  ✓ Works well with clear margin of separation

DISADVANTAGES:
  ✗ Slow for large datasets O(n² to n³)
  ✗ Sensitive to feature scaling
  ✗ Not directly interpretable
  ✗ Probability estimates are secondary
  ✗ Choosing right kernel/parameters can be difficult

WHEN TO USE SVM:
  - Medium-sized datasets
  - High-dimensional data
  - Need non-linear decision boundaries
  - Clear margin between classes

WHEN NOT TO USE:
  - Very large datasets (use SGDClassifier)
  - Need interpretability
  - Many classes (use tree-based methods)
""")
```

## Key Points

- **Maximum margin**: Finds hyperplane with largest separation
- **Support vectors**: Points that define the margin
- **C parameter**: Controls margin vs. error tradeoff
- **Kernel trick**: Maps to higher dimensions implicitly
- **RBF kernel**: Most common, works well generally
- **Gamma**: Controls influence of each training point
- **Scale features**: Required for SVM to work properly

## Reflection Questions

1. Why are only support vectors important for the decision boundary?
2. How does the kernel trick make non-linear SVM efficient?
3. When would you choose a polynomial kernel over RBF?
