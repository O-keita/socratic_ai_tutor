# GPT and Autoregressive Models

## Introduction

GPT (Generative Pre-trained Transformer) models use decoder-only transformers for autoregressive language modeling. They generate text by predicting one token at a time.

## Autoregressive Language Modeling

```python
import numpy as np
import pandas as pd

print("=== AUTOREGRESSIVE MODELING ===")
print("""
GOAL: Model P(x_1, x_2, ..., x_n)

Using chain rule:
  P(x_1,...,x_n) = P(x_1) × P(x_2|x_1) × P(x_3|x_1,x_2) × ...
  
Each token conditioned on ALL previous tokens.

Training: Predict next token at each position
  Input:  "The cat sat on the"
  Target: "cat sat on the mat"
  
  Position 1: P("cat" | "The")
  Position 2: P("sat" | "The cat")
  Position 3: P("on" | "The cat sat")
  etc.

Loss: Cross-entropy at each position
""")

def autoregressive_probability(model_probs, tokens):
    """Calculate sequence probability"""
    log_prob = 0
    for i, token in enumerate(tokens):
        context = tokens[:i] if i > 0 else ['<START>']
        # Get P(token | context) from model
        prob = model_probs.get((tuple(context), token), 0.01)
        log_prob += np.log(prob)
        print(f"  P('{token}' | {context}) = {prob:.3f}")
    
    return np.exp(log_prob)

# Simulated probabilities
model_probs = {
    (('<START>',), 'The'): 0.2,
    (('The',), 'cat'): 0.3,
    (('The', 'cat'), 'sat'): 0.4,
}

print("Probability decomposition:")
tokens = ['The', 'cat', 'sat']
total_prob = autoregressive_probability(model_probs, tokens)
print(f"\nTotal sequence probability: {total_prob:.5f}")
```

## GPT Architecture

```python
print("\n=== GPT ARCHITECTURE ===")
print("""
GPT = DECODER-ONLY Transformer

Key differences from original Transformer:
  1. No encoder - only decoder stack
  2. No cross-attention (nothing to cross to)
  3. Only masked self-attention

Architecture:
  Token Embeddings + Positional Embeddings
           ↓
  ┌────────────────────┐
  │  Masked Multi-Head │
  │   Self-Attention   │
  │        ↓           │
  │   Add & Norm       │
  │        ↓           │
  │   Feed-Forward     │
  │        ↓           │
  │   Add & Norm       │
  └────────────────────┘
         × N layers
           ↓
      Layer Norm
           ↓
    Linear (to vocab)
           ↓
      Softmax → P(next token)

GPT-2: N=12 (small) to 48 (XL)
GPT-3: N=96, d_model=12288
""")
```

## Pre-training Objective

```python
print("\n=== PRE-TRAINING ===")
print("""
CAUSAL LANGUAGE MODELING:

For sequence x_1, x_2, ..., x_n:
  Loss = -Σ log P(x_i | x_1, ..., x_{i-1})

Trained on MASSIVE text corpora:
  - Books
  - Wikipedia
  - Web pages
  - Code

No labeled data needed! Self-supervised learning.
The next token IS the label.

Training scale (GPT-3):
  - 175 billion parameters
  - 300 billion tokens
  - ~$4.6M compute cost
""")

def causal_lm_loss(logits, targets):
    """Causal language modeling loss"""
    # logits: (batch, seq_len, vocab_size)
    # targets: (batch, seq_len)
    
    # Shift: predict position i from positions 0..i-1
    shift_logits = logits[:, :-1, :]
    shift_targets = targets[:, 1:]
    
    # Cross-entropy loss
    # (In practice, use nn.CrossEntropyLoss)
    
    # For each position, compute -log P(target)
    # Average over all positions and batch
    
    return "Cross-entropy loss averaged over positions"

print("Pre-training creates a 'foundation model':")
print("  - Learns language structure")
print("  - Learns world knowledge")
print("  - Learns reasoning patterns")
print("  - Can be fine-tuned for specific tasks")
```

## GPT Evolution

```python
print("\n=== GPT EVOLUTION ===")
print("""
GPT-1 (2018):
  - 117M parameters
  - 12 layers
  - Pre-train + fine-tune paradigm

GPT-2 (2019):
  - 1.5B parameters (XL)
  - Showed zero-shot capabilities
  - "Too dangerous to release" (initially)

GPT-3 (2020):
  - 175B parameters
  - Few-shot learning via prompting
  - No fine-tuning needed for many tasks

GPT-4 (2023):
  - Multimodal (text + images)
  - Improved reasoning
  - RLHF training

Parameter comparison:
  GPT-1:   117M    (1×)
  GPT-2:   1.5B    (13×)
  GPT-3:   175B    (1500×)
  GPT-4:   ~1.7T?  (estimated)
""")

models = {
    'GPT-1': {'params': 117, 'layers': 12, 'context': 512},
    'GPT-2 Small': {'params': 124, 'layers': 12, 'context': 1024},
    'GPT-2 XL': {'params': 1500, 'layers': 48, 'context': 1024},
    'GPT-3': {'params': 175000, 'layers': 96, 'context': 2048},
}

print("\nModel specifications (params in millions):")
for name, specs in models.items():
    print(f"  {name}: {specs['params']:,}M params, {specs['layers']} layers, {specs['context']} context")
```

## In-Context Learning

```python
print("\n=== IN-CONTEXT LEARNING ===")
print("""
GPT-3's key discovery: Learning from examples in the prompt!

ZERO-SHOT: No examples
  Prompt: "Translate English to French: Hello →"
  Output: "Bonjour"

ONE-SHOT: One example
  Prompt: "Translate English to French:
           Hello → Bonjour
           World →"
  Output: "Monde"

FEW-SHOT: Multiple examples
  Prompt: "Sentiment: positive or negative
           Review: Great movie! → positive
           Review: Terrible acting → negative
           Review: Best film ever →"
  Output: "positive"

No weight updates! All in the forward pass.
Model uses attention to pattern-match examples.
""")

def few_shot_prompt(task, examples, query):
    """Construct few-shot prompt"""
    prompt = f"{task}\n\n"
    for inp, out in examples:
        prompt += f"{inp} → {out}\n"
    prompt += f"{query} →"
    return prompt

task = "Classify sentiment as positive or negative"
examples = [
    ("I love this product!", "positive"),
    ("Worst purchase ever", "negative"),
    ("Absolutely fantastic", "positive"),
]
query = "Not worth the money"

prompt = few_shot_prompt(task, examples, query)
print("Few-shot prompt:")
print("-" * 40)
print(prompt)
print("-" * 40)
```

## Text Generation

```python
print("\n=== TEXT GENERATION ===")
print("""
Autoregressive generation:

1. Start with prompt
2. Get P(next token | prompt)
3. Sample or select token
4. Append to sequence
5. Repeat until done

Stopping conditions:
  - End-of-sequence token
  - Maximum length
  - Special stop sequences
""")

def generate_text(prompt_tokens, model_fn, max_new_tokens=20, temperature=1.0):
    """Simplified text generation"""
    tokens = list(prompt_tokens)
    
    for _ in range(max_new_tokens):
        # Get probabilities for next token
        logits = model_fn(tokens)
        
        # Apply temperature
        scaled_logits = logits / temperature
        probs = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits))
        
        # Sample next token
        next_token = np.random.choice(len(probs), p=probs)
        
        # Check for end token
        if next_token == 0:  # Assume 0 is <END>
            break
            
        tokens.append(next_token)
    
    return tokens

print("Generation parameters:")
print("  - temperature: Controls randomness (lower = more deterministic)")
print("  - top_k: Sample from top K tokens only")
print("  - top_p: Sample from tokens covering P probability mass")
print("  - repetition_penalty: Discourage repeating tokens")
```

## Prompt Engineering

```python
print("\n=== PROMPT ENGINEERING ===")
print("""
The art of crafting effective prompts:

1. CLEAR INSTRUCTIONS
   Bad: "Write something about dogs"
   Good: "Write a 3-paragraph essay about the history of dog domestication"

2. FORMAT SPECIFICATION
   "Output your answer as a JSON object with keys: 'summary', 'sentiment', 'confidence'"

3. ROLE PLAYING
   "You are an expert data scientist. Explain the concept of..."

4. STEP-BY-STEP REASONING
   "Let's think step by step..."
   (Chain-of-Thought prompting)

5. EXAMPLES (Few-shot)
   Provide input-output pairs to demonstrate the pattern

6. CONSTRAINTS
   "Keep your response under 100 words"
   "Do not include any code"
""")

print("""
Chain-of-Thought example:

Prompt: "A juggler has 16 balls. Half are golf balls, and half 
         of the golf balls are blue. How many blue golf balls?
         Let's think step by step."

Response: "Step 1: Find number of golf balls
           16 / 2 = 8 golf balls
           
           Step 2: Find blue golf balls  
           8 / 2 = 4 blue golf balls
           
           Answer: 4 blue golf balls"
""")
```

## Key Points

- **Autoregressive**: Predict next token from previous tokens
- **Decoder-only**: GPT uses only transformer decoder
- **Causal masking**: Can only attend to past tokens
- **Pre-training**: Massive unsupervised language modeling
- **In-context learning**: Learn from examples in prompt
- **Prompt engineering**: Craft prompts for best results

## Reflection Questions

1. Why does autoregressive modeling work well for text generation?
2. How can GPT do new tasks without fine-tuning?
3. What are the trade-offs of larger model sizes?
