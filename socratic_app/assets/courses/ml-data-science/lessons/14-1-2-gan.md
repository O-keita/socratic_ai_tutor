# Generative Adversarial Networks (GANs)

## Introduction

GANs learn to generate realistic data through a two-player game between a generator and discriminator. They revolutionized image synthesis and generative modeling.

## The GAN Framework

```python
import numpy as np
import pandas as pd

print("=== GAN CONCEPT ===")
print("""
TWO NEURAL NETWORKS:

GENERATOR (G):
  - Takes random noise z
  - Produces fake samples
  - Goal: Fool the discriminator

DISCRIMINATOR (D):
  - Takes real or fake samples
  - Outputs probability of being real
  - Goal: Distinguish real from fake

Minimax game:
  min_G max_D  E[log D(x)] + E[log(1 - D(G(z)))]
  
  D maximizes: Correctly classify real and fake
  G minimizes: Make D think fake samples are real
""")

print("""
Training dynamics:

   Random noise z
        ↓
   ┌─────────┐
   │Generator│ → Fake samples
   └─────────┘       ↓
                ┌───────────────┐
  Real samples → │ Discriminator │ → Real/Fake
                └───────────────┘
                      ↓
               Classification loss
                      ↓
           Backprop to both G and D
""")
```

## GAN Training

```python
print("\n=== TRAINING PROCEDURE ===")
print("""
For each iteration:

1. TRAIN DISCRIMINATOR:
   a) Sample real data batch
   b) Sample noise, generate fake batch
   c) D predicts on both
   d) Compute loss: -[log D(real) + log(1 - D(fake))]
   e) Update D weights

2. TRAIN GENERATOR:
   a) Sample noise
   b) Generate fake samples
   c) D predicts on fake
   d) Compute loss: -log D(fake)  or  log(1 - D(fake))
   e) Update G weights (not D!)

Alternate these steps throughout training.
""")

def discriminator_loss(D_real, D_fake):
    """Binary cross-entropy loss for D"""
    real_loss = -np.mean(np.log(D_real + 1e-8))
    fake_loss = -np.mean(np.log(1 - D_fake + 1e-8))
    return real_loss + fake_loss

def generator_loss(D_fake):
    """Generator loss (non-saturating)"""
    # Original: log(1 - D(G(z))) - but saturates early
    # Non-saturating: -log(D(G(z)))
    return -np.mean(np.log(D_fake + 1e-8))

# Example losses
D_real = np.array([0.9, 0.8, 0.85])  # D thinks real is real
D_fake = np.array([0.2, 0.3, 0.25])  # D thinks fake is fake

print("Good discriminator scenario:")
print(f"  D(real) = {D_real.mean():.2f}, D(fake) = {D_fake.mean():.2f}")
print(f"  D loss: {discriminator_loss(D_real, D_fake):.3f}")
print(f"  G loss: {generator_loss(D_fake):.3f}")

D_fake_better = np.array([0.6, 0.7, 0.65])  # G improving
print("\nGenerator improving:")
print(f"  D(fake) = {D_fake_better.mean():.2f}")
print(f"  G loss: {generator_loss(D_fake_better):.3f} (lower is better)")
```

## Simple GAN Implementation

```python
print("\n=== GAN IN KERAS ===")
print("""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Generator
def make_generator(latent_dim):
    model = keras.Sequential([
        layers.Dense(256, input_dim=latent_dim),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        
        layers.Dense(512),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        
        layers.Dense(1024),
        layers.LeakyReLU(0.2),
        layers.BatchNormalization(),
        
        layers.Dense(28*28*1, activation='tanh'),
        layers.Reshape((28, 28, 1))
    ])
    return model

# Discriminator
def make_discriminator():
    model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28, 1)),
        
        layers.Dense(512),
        layers.LeakyReLU(0.2),
        layers.Dropout(0.3),
        
        layers.Dense(256),
        layers.LeakyReLU(0.2),
        layers.Dropout(0.3),
        
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# Create models
latent_dim = 100
generator = make_generator(latent_dim)
discriminator = make_discriminator()

# Compile discriminator
discriminator.compile(
    loss='binary_crossentropy',
    optimizer=keras.optimizers.Adam(0.0002, 0.5)
)

# Combined model (for training generator)
discriminator.trainable = False
gan_input = keras.Input(shape=(latent_dim,))
fake_image = generator(gan_input)
gan_output = discriminator(fake_image)
gan = keras.Model(gan_input, gan_output)
gan.compile(loss='binary_crossentropy',
            optimizer=keras.optimizers.Adam(0.0002, 0.5))
""")
```

## Training Loop

```python
print("\n=== TRAINING LOOP ===")
print("""
def train_gan(epochs, batch_size):
    for epoch in range(epochs):
        for _ in range(len(X_train) // batch_size):
            
            # ---------------------
            # Train Discriminator
            # ---------------------
            
            # Sample real images
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            real_images = X_train[idx]
            
            # Generate fake images
            noise = np.random.normal(0, 1, (batch_size, latent_dim))
            fake_images = generator.predict(noise)
            
            # Labels
            real_labels = np.ones((batch_size, 1))
            fake_labels = np.zeros((batch_size, 1))
            
            # Train D on real and fake separately
            d_loss_real = discriminator.train_on_batch(real_images, real_labels)
            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)
            d_loss = 0.5 * (d_loss_real + d_loss_fake)
            
            # ---------------------
            # Train Generator
            # ---------------------
            
            noise = np.random.normal(0, 1, (batch_size, latent_dim))
            # We want G to fool D, so labels are "real" (1)
            misleading_labels = np.ones((batch_size, 1))
            
            g_loss = gan.train_on_batch(noise, misleading_labels)
        
        print(f'Epoch {epoch}: D loss = {d_loss:.4f}, G loss = {g_loss:.4f}')
""")
```

## Training Challenges

```python
print("\n=== GAN TRAINING CHALLENGES ===")
print("""
1. MODE COLLAPSE
   - G produces limited variety
   - All samples look similar
   - D can't differentiate, but diversity lost
   
   Solutions: Minibatch discrimination, unrolled GAN

2. TRAINING INSTABILITY
   - Oscillating losses
   - D too strong → G can't learn
   - G too strong → D gives no signal
   
   Solutions: Spectral normalization, two-timescale

3. VANISHING GRADIENTS
   - Original loss saturates when D is good
   - G gets no learning signal
   
   Solution: Non-saturating loss, WGAN

4. EVALUATION DIFFICULTY
   - No single metric
   - FID, IS commonly used
   - Visual inspection important
""")

print("""
Tips for stable training:

1. Use non-saturating G loss: -log(D(G(z)))
2. LeakyReLU instead of ReLU
3. Batch normalization in G (not always D)
4. Lower learning rate (0.0002)
5. Adam with β₁=0.5
6. Label smoothing (0.9 instead of 1.0)
7. Train D more steps per G step if needed
""")
```

## GAN Variants

```python
print("\n=== POPULAR GAN VARIANTS ===")
print("""
DCGAN (Deep Convolutional GAN):
  - Use convolutions instead of fully connected
  - Upsampling in G, strided conv in D
  - BatchNorm, no pooling
  - Foundation for image GANs

WGAN (Wasserstein GAN):
  - Different loss: Wasserstein distance
  - More stable training
  - D outputs score (not probability)
  - Weight clipping or gradient penalty

WGAN-GP (with Gradient Penalty):
  - Replace weight clipping with gradient penalty
  - Better performance
  - L = E[D(fake)] - E[D(real)] + λ × GP

Conditional GAN (cGAN):
  - Condition on class labels
  - G(z, y) and D(x, y)
  - Generate specific classes

StyleGAN:
  - State-of-the-art face generation
  - Mapping network + style injection
  - Controllable attributes
""")

print("""
Architecture evolution:

GAN (2014)      → FC layers, unstable
DCGAN (2016)    → Convolutions, better images
WGAN (2017)     → Stable training
Progressive (2018) → High resolution
StyleGAN (2019) → Photorealistic faces
StyleGAN3 (2021) → Alias-free generation
""")
```

## Conditional Generation

```python
print("\n=== CONDITIONAL GAN ===")
print("""
Generate specific categories:

G(z, condition) → image matching condition

Example: Generate digit "7"
  z = random noise (100-dim)
  y = one-hot encoded "7"
  
  G takes concatenation [z; y]
  D takes [image; y] and outputs real/fake

Code sketch:
  # Generator input
  noise = Input(shape=(latent_dim,))
  label = Input(shape=(num_classes,))
  g_input = Concatenate()([noise, label])
  
  # Discriminator input  
  image = Input(shape=(28, 28, 1))
  label = Input(shape=(num_classes,))
  label_embedding = Reshape((28, 28, 1))(Dense(28*28)(label))
  d_input = Concatenate()([image, label_embedding])
""")
```

## Evaluation Metrics

```python
print("\n=== EVALUATING GANs ===")
print("""
1. INCEPTION SCORE (IS):
   - Uses pretrained Inception network
   - Measures quality and diversity
   - Higher is better
   
   IS = exp(E[KL(p(y|x) || p(y))])

2. FRÉCHET INCEPTION DISTANCE (FID):
   - Compare statistics of real vs generated
   - Uses Inception features
   - Lower is better
   
   FID = ||μ_r - μ_g||² + Tr(Σ_r + Σ_g - 2(Σ_r Σ_g)^0.5)

3. LPIPS (Perceptual similarity):
   - Diversity measure
   - Uses deep features
   
4. VISUAL INSPECTION:
   - Still important!
   - Check for artifacts
   - Evaluate coherence
""")

print("""
FID comparison (ImageNet 256×256):

  Real images: 0 (by definition)
  BigGAN:     7.4
  StyleGAN2:  2.84
  VQGAN:      5.2
  
Lower FID = closer to real data distribution
""")
```

## Key Points

- **GAN**: Generator vs Discriminator adversarial game
- **Training**: Alternate updating D then G
- **Mode collapse**: G produces limited variety
- **DCGAN**: Convolutional architecture for images
- **WGAN**: Wasserstein distance for stability
- **Conditional GAN**: Generate specific classes
- **FID**: Standard evaluation metric

## Reflection Questions

1. Why is GAN training often unstable compared to other neural networks?
2. How does the non-saturating loss help generator training?
3. What's the trade-off between mode collapse and diversity?
